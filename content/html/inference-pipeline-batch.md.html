<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>
  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <title>Run Batch Transforms with Inference Pipelines</title>
  <style>
      code{white-space: pre-wrap;}
      span.smallcaps{font-variant: small-caps;}
      span.underline{text-decoration: underline;}
      div.column{display: inline-block; vertical-align: top; width: 50%;}
  </style>
  <!--[if lt IE 9]>
    <script src="//cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7.3/html5shiv-printshiv.min.js"></script>
  <![endif]-->
</head>
<body>
<p>
Copyright Amazon.com, Inc. or its affiliates. All Rights Reserved.<br>SPDX-License-Identifier: CC-BY-SA-4.0
</p>
<header id="title-block-header">
<h1 class="title">Run Batch Transforms with Inference Pipelines<a name="inference-pipeline-batch"></a></h1>
</header>
<p>To get inferences on an entire dataset you run a batch transform on a trained model , To run inferences on a full dataset, you can use the same inference pipeline model created and deployed to an endpoint for real-time processing in a batch transform job. To run a batch transform job in a pipeline, you download the input data from Amazon S3 and send it in one or more HTTP requests to the inference pipeline model. For an example that shows how to prepare data for a batch transform, see the “Preparing Data for Batch Transform” section of the <a href="https://github.com/awslabs/amazon-sagemaker-examples/tree/master/advanced_functionality/inference_pipeline_sparkml_xgboost_abalone">ML Pipeline with SparkML and XGBoost - Training and Inference sample notebook</a>. For information about Amazon SageMaker batch transforms, see <a href="how-it-works-batch.md">Get Inferences for an Entire Dataset with Batch Transform</a>.</p>
<p><strong>Note</strong><br />
To use custom Docker images in a pipeline that includes <a href="https://docs.aws.amazon.com/sagemaker/latest/dg/sagemaker-algo-docker-registry-paths.html">Amazon SageMaker built-in algorithms</a>, you need an <a href="https://docs.aws.amazon.com/AmazonECR/latest/userguide/what-is-ecr.html">Amazon Elastic Container Registry (Amazon ECR) policy</a>. Your Amazon ECR repository must grant Amazon SageMaker permission to pull the image. For more information, see <a href="inference-pipeline-troubleshoot.md#inference-pipeline-troubleshoot-permissions">Troubleshoot Amazon ECR Permissions for Inference Pipelines</a>.</p>
<p>The following example shows how to run a transform job using the Amazon SageMaker Python SDK. In this example, <code>model_name</code> is the inference pipeline that combines SparkML and XGBoost models (created in previous examples). The Amazon S3 location specified by <code>input_data_path</code> contains the input data, in CSV format, to be downloaded and sent to the Spark ML model. After the transform job has finished, the Amazon S3 location specified by <code>output_data_path</code> contains the output data returned by the XGBoost model in CVS format.</p>
<pre><code>input_data_path = &#39;s3://{}/{}/{}&#39;.format(default_bucket, &#39;key&#39;, &#39;file_name&#39;)
output_data_path = &#39;s3://{}/{}&#39;.format(default_bucket, &#39;key&#39;)
transform_job = sagemaker.transformer.Transformer(
    model_name = model_name,
    instance_count = 1,
    instance_type = &#39;ml.m4.xlarge&#39;,
    strategy = &#39;SingleRecord&#39;,
    assemble_with = &#39;Line&#39;,
    output_path = output_data_path,
    base_transform_job_name=&#39;inference-pipelines-batch&#39;,
    sagemaker_session=sess,
    accept = CONTENT_TYPE_CSV)
transform_job.transform(data = input_data_path, 
                        content_type = CONTENT_TYPE_CSV, 
                        split_type = &#39;Line&#39;)</code></pre>
</body>
</html>
