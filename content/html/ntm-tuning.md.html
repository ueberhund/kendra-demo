<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>
  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <title>Tune an NTM Model</title>
  <style>
      code{white-space: pre-wrap;}
      span.smallcaps{font-variant: small-caps;}
      span.underline{text-decoration: underline;}
      div.column{display: inline-block; vertical-align: top; width: 50%;}
  </style>
  <!--[if lt IE 9]>
    <script src="//cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7.3/html5shiv-printshiv.min.js"></script>
  <![endif]-->
</head>
<body>
<p>
Copyright Amazon.com, Inc. or its affiliates. All Rights Reserved.<br>SPDX-License-Identifier: CC-BY-SA-4.0
</p>
<header id="title-block-header">
<h1 class="title">Tune an NTM Model<a name="ntm-tuning"></a></h1>
</header>
<p><em>Automatic model tuning</em>, also known as hyperparameter tuning, finds the best version of a model by running many jobs that test a range of hyperparameters on your dataset. You choose the tunable hyperparameters, a range of values for each, and an objective metric. You choose the objective metric from the metrics that the algorithm computes. Automatic model tuning searches the hyperparameters chosen to find the combination of values that result in the model that optimizes the objective metric.</p>
<p>Amazon SageMaker NTM is an unsupervised learning algorithm that learns latent representations of large collections of discrete data, such as a corpus of documents. Latent representations use inferred variables that are not directly measured to model the observations in a dataset. Automatic model tuning on NTM helps you find the model that minimizes loss over the training or validation data. <em>Training loss</em> measures how well the model fits the training data. <em>Validation loss</em> measures how well the model can generalize to data that it is not trained on. Low training loss indicates that a model is a good fit to the training data. Low validation loss indicates that a model has not overfit the training data and so should be able to model documents on which is has not been trained successfully. Usually, it’s preferable to have both losses be small. However, minimizing training loss too much might result in overfitting and increase validation loss, which would reduce the generality of the model.</p>
<p>For more information about model tuning, see <a href="automatic-model-tuning.md">Automatic Model Tuning</a>.</p>
<p>The NTM algorithm reports a single metric that is computed during training: <code>validation:total_loss</code>. The total loss is the sum of the reconstruction loss and Kullback-Leibler divergence. When tuning hyperparameter values, choose this metric as the objective.</p>
<table>
<thead>
<tr class="header">
<th>Metric Name</th>
<th>Description</th>
<th>Optimization Direction</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>validation:total_loss</td>
<td>Total Loss on validation set</td>
<td>Minimize</td>
</tr>
</tbody>
</table>
<p>You can tune the following hyperparameters for the NTM algorithm. Usually setting low <code>mini_batch_size</code> and small <code>learning_rate</code> values results in lower validation losses, although it might take longer to train. Low validation losses don’t necessarily produce more coherent topics as interpreted by humans. The effect of other hyperparameters on training and validation loss can vary from dataset to dataset. To see which values are compatible, see <a href="ntm_hyperparameters.md">NTM Hyperparameters</a>.</p>
<table>
<colgroup>
<col style="width: 33%" />
<col style="width: 33%" />
<col style="width: 33%" />
</colgroup>
<thead>
<tr class="header">
<th>Parameter Name</th>
<th>Parameter Type</th>
<th>Recommended Ranges</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>encoder_layers_activation</td>
<td>CategoricalParameterRanges</td>
<td>[‘sigmoid’, ‘tanh’, ‘relu’]</td>
</tr>
<tr class="even">
<td>learning_rate</td>
<td>ContinuousParameterRange</td>
<td>MinValue: 1e-4, MaxValue: 0.1</td>
</tr>
<tr class="odd">
<td>mini_batch_size</td>
<td>IntegerParameterRanges</td>
<td>MinValue: 16, MaxValue:2048</td>
</tr>
<tr class="even">
<td>optimizer</td>
<td>CategoricalParameterRanges</td>
<td>[‘sgd’, ‘adam’, ‘adadelta’]</td>
</tr>
<tr class="odd">
<td>rescale_gradient</td>
<td>ContinuousParameterRange</td>
<td>MinValue: 0.1, MaxValue: 1.0</td>
</tr>
<tr class="even">
<td>weight_decay</td>
<td>ContinuousParameterRange</td>
<td>MinValue: 0.0, MaxValue: 1.0</td>
</tr>
</tbody>
</table>
</body>
</html>
