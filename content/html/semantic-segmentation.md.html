<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>
  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <title>Semantic Segmentation Algorithm</title>
  <style>
      code{white-space: pre-wrap;}
      span.smallcaps{font-variant: small-caps;}
      span.underline{text-decoration: underline;}
      div.column{display: inline-block; vertical-align: top; width: 50%;}
  </style>
  <!--[if lt IE 9]>
    <script src="//cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7.3/html5shiv-printshiv.min.js"></script>
  <![endif]-->
</head>
<body>
<p>
Copyright Amazon.com, Inc. or its affiliates. All Rights Reserved.<br>SPDX-License-Identifier: CC-BY-SA-4.0
</p>
<header id="title-block-header">
<h1 class="title">Semantic Segmentation Algorithm<a name="semantic-segmentation"></a></h1>
</header>
<p>The Amazon SageMaker semantic segmentation algorithm provides a fine-grained, pixel-level approach to developing computer vision applications. It tags every pixel in an image with a class label from a predefined set of classes. Tagging is fundamental for understanding scenes, which is critical to an increasing number of computer vision applications, such as self-driving vehicles, medical imaging diagnostics, and robot sensing.</p>
<p>For comparison, the Amazon SageMaker <a href="image-classification.md">Image Classification Algorithm</a> is a supervised learning algorithm that analyzes only whole images, classifying them into one of multiple output categories. The <a href="object-detection.md">Object Detection Algorithm</a> is a supervised learning algorithm that detects and classifies all instances of an object in an image. It indicates the location and scale of each object in the image with a rectangular bounding box.</p>
<p>Because the semantic segmentation algorithm classifies every pixel in an image, it also provides information about the shapes of the objects contained in the image. The segmentation output is represented as an RGB or grayscale image, called a <em>segmentation mask</em>. A segmentation mask is an RGB (or grayscale) image with the same shape as the input image.</p>
<p>Amazon SageMaker semantic segmentation algorithm is built using the <a href="https://github.com/dmlc/gluon-cv">MXNet Gluon framework and the Gluon CV toolkit</a> provides you with a choice of three build-in algorithms to train a deep neural network. You can use the <a href="https://arxiv.org/abs/1605.06211">Fully-Convolutional Network (FCN) algorithm</a>, <a href="https://arxiv.org/abs/1612.01105">Pyramid Scene Parsing (PSP) algorithm</a>, or <a href="https://arxiv.org/abs/1706.05587">DeepLabV3</a>.</p>
<p>Each of the three algorithms has two distinct components: + The <em>backbone</em> (or <em>encoder</em>)—A network that produces reliable activation maps of features. + The <em>decoder</em>—A network that constructs the segmentation mask from the encoded activation maps.</p>
<p>You also have a choice of backbones for the FCN, PSP, and DeepLabV3 algorithms: <a href="https://arxiv.org/abs/1512.03385">ResNet50 or ResNet101</a>. These backbones include pretrained artifacts that were originally trained on the <a href="http://www.image-net.org/">ImageNet</a> classification task. You can fine-tune these backbones for segmentation using your own data. Or, you can initialize and train these networks from scratch using only your own data. The decoders are never pretrained.</p>
<p>To deploy the trained model for inference, use the Amazon SageMaker hosting service. During inference, you can request the segmentation mask either as a PNG image or as a set of probabilities for each class for each pixel. You can use these masks as part of a larger pipeline that includes additional downstream image processing or other applications.</p>
<p><strong>Topics</strong> + <a href="#semantic-segmentation-sample-notebooks">Semantic Segmentation Sample Notebooks</a> + <a href="#semantic-segmentation-inputoutput">Input/Output Interface for the Semantic Segmentation Algorithm</a> + <a href="#semantic-segmentation-instances">EC2 Instance Recommendation for the Semantic Segmentation Algorithm</a> + <a href="segmentation-hyperparameters.md">Semantic Segmentation Hyperparameters</a></p>
<p>For a sample Jupyter notebook that uses the Amazon SageMaker semantic segmentation algorithm to train a model and deploy it to perform inferences, see the <a href="https://github.com/awslabs/amazon-sagemaker-examples/blob/master/introduction_to_amazon_algorithms/semantic_segmentation_pascalvoc/semantic_segmentation_pascalvoc.ipynb">Semantic Segmentation Example</a>. For instructions on how to create and access Jupyter notebook instances that you can use to run the example in Amazon SageMaker, see <a href="nbi.md">Use Notebook Instances</a>.</p>
<p>To see a list of all of the Amazon SageMaker samples, create and open a notebook instance, and choose the <strong>SageMaker Examples</strong> tab. The example semantic segmentation notebooks are located under <strong>Introduction to Amazon algorithms</strong>. To open a notebook, choose its <strong>Use</strong> tab, and choose <strong>Create copy</strong>.</p>
<p>Amazon SageMaker semantic segmentation expects the customer’s training dataset to be on <a href="https://aws.amazon.com/s3/">Amazon Simple Storage Service (Amazon S3)</a>. Once trained, it produces the resulting model artifacts on Amazon S3. The input interface format for the Amazon SageMaker semantic segmentation is similar to that of most standardized semantic segmentation benchmarking datasets. The dataset in Amazon S3 is expected to be presented in two channels, one for <code>train</code> and one for <code>validation</code> using four directories, two for images and two for annotations. Annotations are expected to be uncompressed PNG images. The dataset might also have a label map that describes how the annotation mappings are established. If not, the algorithm uses a default. It also supports the augmented manifest image format (<code>application/x-image</code>) for training in Pipe input mode straight from Amazon S3. For inference, an endpoint accepts images with an <code>image/jpeg</code> content type.</p>
<p>The training data is split into four directories: <code>train</code>, <code>train_annotation</code>, <code>validation</code>, and <code>validation_annotation</code>. There is a channel for each of these directories. The dataset also expected to have one <code>label_map.json</code> file per channel for <code>train_annotation</code> and <code>validation_annotation</code> respectively. If you don’t provide these JSON files, Amazon SageMaker provides the default set label map.</p>
<p>The dataset specifying these files should look similar to the following example:</p>
<pre><code>s3://bucket_name
    |
    |- train
                 |
                 | - 0000.jpg
                 | - coffee.jpg
    |- validation
                 |
                 | - 00a0.jpg
                 | - bananna.jpg              
    |- train_annotation
                 |
                 | - 0000.png
                 | - coffee.png
    |- validation_annotation
                 |
                 | - 00a0.png   
                 | - bananna.png 
    |- label_map
                 | - train_label_map.json  
                 | - validation_label_map.json</code></pre>
<p>Every JPG image in the train and validation directories has a corresponding PNG label image with the same name in the <code>train_annotation</code> and <code>validation_annotation</code> directories. This naming convention helps the algorithm to associate a label with its corresponding image during training. The <code>train</code>, <code>train_annotation</code>, <code>validation</code>, and <code>validation_annotation</code> channels are mandatory. The annotations are single-channel PNG images. The format works as long as the metadata (modes) in the image helps the algorithm read the annotation images into a single-channel 8-bit unsigned integer. For more information on our support for modes, see the <a href="https://pillow.readthedocs.io/en/3.0.x/handbook/concepts.html#modes">Python Image Library documentation</a>. We recommend using the 8-bit pixel, true color <code>P</code> mode.</p>
<p>The image that is encoded is a simple 8-bit integer when using modes. To get from this mapping to a map of a label, the algorithm uses one mapping file per channel, called the <em>label map</em>. The label map is used to map the values in the image with actual label indices. In the default label map, which is provided by default if you don’t provide one, the pixel value in an annotation matrix (image) directly index the label. These images can be grayscale PNG files or 8-bit indexed PNG files. The label map file for the unscaled default case is the following:</p>
<pre><code>{ 
  &quot;scale&quot;: &quot;1&quot;
}</code></pre>
<p>To provide some contrast for viewing, some annotation software scales the label images by a constant amount. To support this, the Amazon SageMaker semantic segmentation algorithm provides a rescaling option to scale down the values to actual label values. When scaling down doesn’t convert the value to an appropriate integer, the algorithm defaults to the greatest integer less than or equal to the scale value. The following code shows how to set the scale value to rescale the label values:</p>
<pre><code>{ 
  &quot;scale&quot;: &quot;3&quot;
}</code></pre>
<p>The following example shows how this <code>"scale"</code> value is used to rescale the <code>encoded_label</code> values of the input annotation image when they are mapped to the <code>mapped_label</code> values to be used in training. The label values in the input annotation image are 0, 3, 6, with scale 3, so they are mapped to 0, 1, 2 for training:</p>
<pre><code>encoded_label = [0, 3, 6]
mapped_label = [0, 1, 2]</code></pre>
<p>In some cases, you might need to specify a particular color mapping for each class. Use the map option in the label mapping as shown in the following example of a <code>label_map</code> file:</p>
<pre><code>{
    &quot;map&quot;: {
        &quot;0&quot;: 5,
        &quot;1&quot;: 0,
        &quot;2&quot;: 2
    }
}</code></pre>
<p>This label mapping for this example is:</p>
<pre><code>encoded_label = [0, 5, 2]
mapped_label = [1, 0, 2]</code></pre>
<p>With label mappings, you can use different annotation systems and annotation software to obtain data without a lot of preprocessing. You can provide one label map per channel. The files for a label map in the <code>label_map</code> channel must follow the naming conventions for the four directory structure. If you don’t provide a label map, the algorithm assumes a scale of 1 (the default).</p>
<p>The augmented manifest format enables you to do training in Pipe mode using image files without needing to create RecordIO files. The augmented manifest file contains data objects and should be in <a href="http://jsonlines.org/">JSON Lines</a> format, as described in the <a href="API_CreateTrainingJob.md">CreateTrainingJob</a> request API. Each line in the manifest is an entry containing the Amazon S3 URI for the image and the URI for the annotation image.</p>
<p>Each JSON object in the manifest file must contain a <code>source-ref</code> key. The <code>source-ref</code> key should contain the value of the Amazon S3 URI to the image. The labels are provided under the <code>AttributeNames</code> parameter value as specified in the <a href="API_CreateTrainingJob.md">CreateTrainingJob</a> request. It can also contain additional metadata under the metadata tag, but these are ignored by the algorithm. In the example below, the <code>AttributeNames</code> are contained in the list of image and annotation references <code>["source-ref", "city-streets-ref"]</code>. These names must have <code>-ref</code> appended to them. When using the Semantic Segmentation algorithm with Augmented Manifest, the value of the <code>RecordWrapperType</code> parameter must be <code>"RecordIO"</code> and value of the <code>ContentType</code> parameter must be <code>application/x-recordio</code>.</p>
<pre><code>{&quot;source-ref&quot;: &quot;S3 bucket location&quot;, &quot;city-streets-ref&quot;: &quot;S3 bucket location&quot;, &quot;city-streets-metadata&quot;: {&quot;job-name&quot;: &quot;label-city-streets&quot;, }}</code></pre>
<p>For more information on augmented manifest files, see <a href="augmented-manifest.md">Provide Dataset Metadata to Training Jobs with an Augmented Manifest File</a>.</p>
<p>You can also seed the training of a new model with a model that you trained previously using Amazon SageMaker. This incremental training saves training time when you want to train a new model with the same or similar data. Currently, incremental training is supported only for models trained with the built-in Amazon SageMaker Semantic Segmentation.</p>
<p>To use your own pre-trained model, specify the <code>ChannelName</code> as “model” in the <code>InputDataConfig</code> for the <a href="API_CreateTrainingJob.md">CreateTrainingJob</a> request. Set the <code>ContentType</code> for the model channel to <code>application/x-sagemaker-model</code>. The <code>backbone</code>, <code>algorithm</code>, <code>crop_size</code>, and <code>num_classes</code> input parameters that define the network architecture must be consistently specified in the input hyperparameters of the new model and the pre-trained model that you upload to the model channel. For the pretrained model file, you can use the compressed (.tar.gz) artifacts from Amazon SageMaker outputs. You can use either RecordIO or Image formats for input data. For more information on incremental training and for instructions on how to use it, see <a href="incremental-training.md">Incremental Training in Amazon SageMaker</a>.</p>
<p>To query a trained model that is deployed to an endpoint, you need to provide an image and an <code>AcceptType</code> that denotes the type of output required. The endpoint takes JPEG images with an <code>image/jpeg</code> content type. If you request an <code>AcceptType</code> of <code>image/png</code>, the algorithm outputs a PNG file with a segmentation mask in the same format as the labels themselves. If you request an accept type of<code>application/x-recordio-protobuf</code>, the algorithm returns class probabilities encoded in recordio-protobuf format. The latter format outputs a 3D tensor where the third dimension is the same size as the number of classes. This component denotes the probability of each class label for each pixel.</p>
<p>The Amazon SageMaker semantic segmentation algorithm only supports GPU instances for training, and we recommend using GPU instances with more memory for training with large batch sizes. The algorithm can be trained using <a href="https://aws.amazon.com/ec2/">P2/P3 EC2 Amazon Elastic Compute Cloud (Amazon EC2)</a> instances in single machine configurations. It supports the following GPU instances for training: + <code>ml.p2.xlarge</code> + <code>ml.p2.8xlarge</code> + <code>ml.p2.16xlarge</code> + <code>ml.p3.2xlarge</code> + <code>ml.p3.8xlarge</code> + <code>ml.p3.16xlarge</code></p>
<p>For inference, you can use either CPU instances (such as c5 and m5) and GPU instances (such as p2 and p3) or both. For information about the instance types that provide varying combinations of CPU, GPU, memory, and networking capacity for inference, see <a href="https://aws.amazon.com/sagemaker/pricing/instance-types/">Amazon SageMaker ML Instance Types</a>.</p>
</body>
</html>
