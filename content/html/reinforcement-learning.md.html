<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>
  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <title>Reinforcement Learning with Amazon SageMaker RL</title>
  <style>
      code{white-space: pre-wrap;}
      span.smallcaps{font-variant: small-caps;}
      span.underline{text-decoration: underline;}
      div.column{display: inline-block; vertical-align: top; width: 50%;}
  </style>
  <!--[if lt IE 9]>
    <script src="//cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7.3/html5shiv-printshiv.min.js"></script>
  <![endif]-->
</head>
<body>
<p>
Copyright Amazon.com, Inc. or its affiliates. All Rights Reserved.<br>SPDX-License-Identifier: CC-BY-SA-4.0
</p>
<header id="title-block-header">
<h1 class="title">Reinforcement Learning with Amazon SageMaker RL<a name="reinforcement-learning"></a></h1>
</header>
<p>Reinforcement learning (RL) is a machine learning technique that attempts to learn a strategy, called a policy, that optimizes an objective for an agent acting in an environment. For example, the agent might be a robot, the environment might be a maze, and the goal might be to successfully navigate the maze in the smallest amount of time. In RL, the agent takes an action, observes the state of the environment, and gets a reward based on the value of the current state of the environment. The goal is to maximize the long-term reward that the agent receives as a result of its actions. RL is well-suited for solving problems where an agent can make autonomous decisions.</p>
<p><strong>Topics</strong> + <a href="#rl-why">Why is Reinforcement Learning Important?</a> + <a href="#rl-terms">Markov Decision Process (MDP)</a> + <a href="#sagemaker-rl">Key Features of Amazon SageMaker RL</a> + <a href="sagemaker-rl-workflow.md">Sample RL Workflow Using Amazon SageMaker RL</a> + <a href="sagemaker-rl-environments.md">RL Environments in Amazon SageMaker</a> + <a href="sagemaker-rl-distributed.md">Distributed Training with Amazon SageMaker RL</a> + <a href="sagemaker-rl-tuning.md">Hyperparameter Tuning with Amazon SageMaker RL</a></p>
<p>RL is well-suited for solving large, complex problems. For example, supply chain management, HVAC systems, industrial robotics, game artificial intelligence, dialog systems, and autonomous vehicles. Because RL models learn by a continuous process of receiving rewards and punishments for every action taken by the agent, it is possible to train systems to make decisions under uncertainty and in dynamic environments.</p>
<p>RL is based on models called Markov Decision Processes (MDPs). An MDP consists of a series of time steps. Each time step consists of the following:</p>
<p>Environment<br />
Defines the space in which the RL model operates. This can be either a real-world environment or a simulator. For example, if you train a physical autonomous vehicle on a physical road, that would be a real-world environment. If you train a computer program that models an autonomous vehicle driving on a road, that would be a simulator.</p>
<p>State<br />
Specifies all information about the environment and past steps that is relevant to the future. For example, in an RL model in which a robot can move in any direction at any time step, then the position of the robot at the current time step is the state, because if we know where the robot is, it isnâ€™t necessary to know the steps it took to get there.</p>
<p>Action<br />
What the agent does. For example, the robot takes a step forward.</p>
<p>Reward<br />
A number that represents the value of the state that resulted from the last action that the agent took. For example, if the goal is for a robot to find treasure, the reward for finding treasure might be 5, and the reward for not finding treasure might be 0. The RL model attempts to find a strategy that optimizes the cumulative reward over the long term. This strategy is called a <em>policy</em>.</p>
<p>Observation<br />
Information about the state of the environment that is available to the agent at each step. This might be the entire state, or it might be just a part of the state. For example, the agent in a chess-playing model would be able to observe the entire state of the board at any step, but a robot in a maze might only be able to observe a small portion of the maze that it currently occupies.</p>
<p>Typically, training in RL consists of many <em>episodes</em>. An episode consists of all of the time steps in an MDP from the initial state until the environment reaches the terminal state.</p>
<p>To train RL models in Amazon SageMaker RL, use the following components: + A deep learning (DL) framework. Currently, Amazon SageMaker supports RL in TensorFlow and Apache MXNet. + An RL toolkit. An RL toolkit manages the interaction between the agent and the environment, and provides a wide selection of state of the art RL algorithms. Amazon SageMaker supports the Intel Coach and Ray RLlib toolkits. For information about Intel Coach, see <a href="https://nervanasystems.github.io/coach/">https://nervanasystems.github.io/coach/</a>. For information about Ray RLlib, see <a href="https://ray.readthedocs.io/en/latest/rllib.html">https://ray.readthedocs.io/en/latest/rllib.html</a>. + An RL environment. You can use custom environments, open-source environments, or commercial environments. For information, see <a href="sagemaker-rl-environments.md">RL Environments in Amazon SageMaker</a>.</p>
<p>The following diagram shows the RL components that are supported in Amazon SageMaker RL.</p>
<figure>
<img src="http://docs.aws.amazon.com/sagemaker/latest/dg/images/sagemaker-rl-support.png" alt="" /><figcaption>[Image NOT FOUND]</figcaption>
</figure>
</body>
</html>
