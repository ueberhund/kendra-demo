<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>
  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <title>Latent Dirichlet Allocation (LDA) Algorithm</title>
  <style>
      code{white-space: pre-wrap;}
      span.smallcaps{font-variant: small-caps;}
      span.underline{text-decoration: underline;}
      div.column{display: inline-block; vertical-align: top; width: 50%;}
  </style>
  <!--[if lt IE 9]>
    <script src="//cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7.3/html5shiv-printshiv.min.js"></script>
  <![endif]-->
</head>
<body>
<p>
Copyright Amazon.com, Inc. or its affiliates. All Rights Reserved.<br>SPDX-License-Identifier: CC-BY-SA-4.0
</p>
<header id="title-block-header">
<h1 class="title">Latent Dirichlet Allocation (LDA) Algorithm<a name="lda"></a></h1>
</header>
<p>The Amazon SageMaker Latent Dirichlet Allocation (LDA) algorithm is an unsupervised learning algorithm that attempts to describe a set of observations as a mixture of distinct categories. LDA is most commonly used to discover a user-specified number of topics shared by documents within a text corpus. Here each observation is a document, the features are the presence (or occurrence count) of each word, and the categories are the topics. Since the method is unsupervised, the topics are not specified up front, and are not guaranteed to align with how a human may naturally categorize documents. The topics are learned as a probability distribution over the words that occur in each document. Each document, in turn, is described as a mixture of topics.</p>
<p>The exact content of two documents with similar topic mixtures will not be the same. But overall, you would expect these documents to more frequently use a shared subset of words, than when compared with a document from a different topic mixture. This allows LDA to discover these word groups and use them to form topics. As an extremely simple example, given a set of documents where the only words that occur within them are: <em>eat</em>, <em>sleep</em>, <em>play</em>, <em>meow</em>, and <em>bark</em>, LDA might produce topics like the following:</p>
<table>
<thead>
<tr class="header">
<th><strong>Topic</strong></th>
<th><em>eat</em></th>
<th><em>sleep</em></th>
<th><em>play</em></th>
<th><em>meow</em></th>
<th><em>bark</em></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Topic 1</td>
<td>0.1</td>
<td>0.3</td>
<td>0.2</td>
<td>0.4</td>
<td>0.0</td>
</tr>
<tr class="even">
<td>Topic 2</td>
<td>0.2</td>
<td>0.1</td>
<td>0.4</td>
<td>0.0</td>
<td>0.3</td>
</tr>
</tbody>
</table>
<p>You can infer that documents that are more likely to fall into Topic 1 are about cats (who are more likely to <em>meow</em> and <em>sleep</em>), and documents that fall into Topic 2 are about dogs (who prefer to <em>play</em> and <em>bark</em>). These topics can be found even though the words dog and cat never appear in any of the texts.</p>
<p><strong>Topics</strong> + <a href="#lda-inputoutput">Input/Output Interface for the LDA Algorithm</a> + <a href="#lda-instances">EC2 Instance Recommendation for the LDA Algorithm</a> + <a href="#LDA-sample-notebooks">LDA Sample LDA Notebooks</a> + <a href="lda-how-it-works.md">How LDA Works</a> + <a href="lda_hyperparameters.md">LDA Hyperparameters</a> + <a href="lda-tuning.md">Tune an LDA Model</a></p>
<p>LDA expects data to be provided on the train channel, and optionally supports a test channel, which is scored by the final model. LDA supports both <code>recordIO-wrapped-protobuf</code> (dense and sparse) and <code>CSV</code> file formats. For <code>CSV</code>, the data must be dense and have dimension equal to <em>number of records * vocabulary size</em>. LDA can be trained in File or Pipe mode when using recordIO-wrapped protobuf, but only in File mode for the <code>CSV</code> format.</p>
<p>For inference, <code>text/csv</code>, <code>application/json</code>, and <code>application/x-recordio-protobuf</code> content types are supported. Sparse data can also be passed for <code>application/json</code> and <code>application/x-recordio-protobuf</code>. LDA inference returns <code>application/json</code> or <code>application/x-recordio-protobuf</code> <em>predictions</em>, which include the <code>topic_mixture</code> vector for each observation.</p>
<p>Please see the <a href="#LDA-sample-notebooks">LDA Sample LDA Notebooks</a> for more detail on training and inference formats.</p>
<p>LDA currently only supports single-instance CPU training. CPU instances are recommended for hosting/inference.</p>
<p>For a sample notebook that shows how to train the Amazon SageMaker Latent Dirichlet Allocation algorithm on a dataset and then how to deploy the trained model to perform inferences about the topic mixtures in input documents, see the <a href="https://github.com/awslabs/amazon-sagemaker-examples/blob/master/introduction_to_amazon_algorithms/lda_topic_modeling/LDA-Introduction.ipynb">An Introduction to SageMaker LDA</a>. For instructions how to create and access Jupyter notebook instances that you can use to run the example in Amazon SageMaker, see <a href="nbi.md">Use Notebook Instances</a>. Once you have created a notebook instance and opened it, select the <strong>SageMaker Examples</strong> tab to see a list of all the Amazon SageMaker samples. The topic modeling example notebooks using the NTM algorithms are located in the <strong>Introduction to Amazon algorithms</strong> section. To open a notebook, click on its <strong>Use</strong> tab and select <strong>Create copy</strong>.</p>
</body>
</html>
