<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>
  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <title>Batch Transform</title>
  <style>
      code{white-space: pre-wrap;}
      span.smallcaps{font-variant: small-caps;}
      span.underline{text-decoration: underline;}
      div.column{display: inline-block; vertical-align: top; width: 50%;}
  </style>
  <!--[if lt IE 9]>
    <script src="//cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7.3/html5shiv-printshiv.min.js"></script>
  <![endif]-->
</head>
<body>
<p>
Copyright Amazon.com, Inc. or its affiliates. All Rights Reserved.<br>SPDX-License-Identifier: CC-BY-SA-4.0
</p>
<header id="title-block-header">
<h1 class="title">Batch Transform<a name="batch-transform"></a></h1>
</header>
<p>To preprocess or get inferences for an entire dataset, use batch transform. Use batch transform when you need to work with large datasets, process datasets quickly or sub-second latency. Use preprocessing to remove noise or bias from your dataset that interferes with training or inference. Use batch transform for inference when you don’t need a persistent endpoint. You can use batch transform for example, to compare production variants that deploy different models.</p>
<p>To filter input data before performing inferences or to associate input records with inferences about those records, use <a href="batch-transform-data-processing.md">Associate Prediction Results with their Corresponding Input Records</a>. This is useful for example, to provide context for creating and interpreting reports about the output data.</p>
<p>For more information about batch transform, see <a href="how-it-works-batch.md">Get Inferences for an Entire Dataset with Batch Transform</a>.</p>
<p><strong>Topics</strong> + <a href="#batch-transform-large-datasets">Use Batch Transform with Large Datasets</a> + <a href="#batch-transform-reduce-time">Speed Up a Batch Transform Job</a> + <a href="#batch-transform-test-variants">Use Batch Transform to Test Production Variants</a> + <a href="#batch-transform-errors">Batch Transform Errors</a> + <a href="#batch-transform-notebooks">Batch Transform Sample Notebooks</a> + <a href="batch-transform-data-processing.md">Associate Prediction Results with their Corresponding Input Records</a></p>
<p>Batch transform automatically manages the processing of large datasets within the limits of specified parameters. For example, suppose that you have a dataset file, <code>input1.csv</code>, stored in an S3 bucket. The content of the input file might look like this:</p>
<pre><code>Record1-Attribute1, Record1-Attribute2, Record1-Attribute3, ..., Record1-AttributeM
Record2-Attribute1, Record2-Attribute2, Record2-Attribute3, ..., Record2-AttributeM
Record3-Attribute1, Record3-Attribute2, Record3-Attribute3, ..., Record3-AttributeM
...
RecordN-Attribute1, RecordN-Attribute2, RecordN-Attribute3, ..., RecordN-AttributeM</code></pre>
<p>When a batch transform job starts, Amazon SageMaker initializes compute instances and distributes the inference or preprocessing workload between them. When you have multiples files, one instance might process <code>input1.csv</code>, and the other instance might process another file named <code>input2.csv</code>. To keep large payloads within the <a href="https://docs.aws.amazon.com/sagemaker/latest/dg/API_CreateTransformJob.html#SageMaker-CreateTransformJob-request-MaxPayloadInMB">MaxPayloadInMB</a> limit, you might split an input file into several mini-batches. For example, you might create a mini-batch created from <code>input1.csv</code>, as follows.</p>
<pre><code>Record3-Attribute1, Record3-Attribute2, Record3-Attribute3, ..., Record3-AttributeM
Record4-Attribute1, Record4-Attribute2, Record4-Attribute3, ..., Record4-AttributeM</code></pre>
<p><strong>Note</strong><br />
Amazon SageMaker processes each input file separately. It doesn’t combine mini-batches from different input files to comply with the <a href="https://docs.aws.amazon.com/sagemaker/latest/dg/API_CreateTransformJob.html#SageMaker-CreateTransformJob-request-MaxPayloadInMB">MaxPayloadInMB</a> limit.</p>
<p>To split input files into mini-batches, when you create a batch transform job, set the <a href="https://docs.aws.amazon.com/sagemaker/latest/dg/API_TransformInput.html#SageMaker-Type-TransformInput-SplitType">SplitType</a> parameter value to <code>Line</code>. If SplitType is set to <code>None</code> or if an input file can’t be split into mini-batches, Amazon SageMaker uses the entire input file in a single request.</p>
<p>If the batch transform job successfully processes all of the records in an input file, it creates an output file with the same name and an <code>.out</code> file extension. For multiple input files, such as <code>input1.csv</code> and <code>input2.csv</code>, the output files are named <code>input1.csv.out</code>, and <code>input2.csv.out</code>. The batch transform job stores the output files in the specified location in Amazon S3, such as <code>s3://awsexamplebucket/output/</code>. The predictions in an output file are listed in the same order as the corresponding records in the input file. The following would be the contents of the output file <code>input1.csv.out</code>, based on the input file shown earlier.</p>
<pre><code>Inference1-Attribute1, Inference1-Attribute2, Inference1-Attribute3, ..., Inference1-AttributeM
Inference2-Attribute1, Inference2-Attribute2, Inference2-Attribute3, ..., Inference2-AttributeM
Inference3-Attribute1, Inference3-Attribute2, Inference3-Attribute3, ..., Inference3-AttributeM
...
InferenceN-Attribute1, Inference3-Attribute2, Inference3-Attribute3, ..., InferenceN-AttributeM</code></pre>
<p>To combine the results of multiple output files into a single output file, set the <a href="https://docs.aws.amazon.com/sagemaker/latest/dg/API_TransformOutput.html#SageMaker-Type-TransformOutput-AssembleWith">AssembleWith</a> parameter to <code>Line</code>.</p>
<p>When the input data is very large and is transmitted using HTTP chunked encoding, to stream the data to the algorithm, set <a href="https://docs.aws.amazon.com/sagemaker/latest/dg/API_CreateTransformJob.html#SageMaker-CreateTransformJob-request-MaxPayloadInMB">MaxPayloadInMB</a> to <code>0</code>. Currently, Amazon SageMaker built-in algorithms don’t support this feature.</p>
<p>For information about using the API to create a batch transform job, see the <a href="API_CreateTransformJob.md">CreateTransformJob</a> API. For more information about the correlation between batch transform input and output objects, see <a href="https://docs.aws.amazon.com/sagemaker/latest/dg/API_OutputDataConfig.html">OutputDataConfig</a>. For an example of how to use batch transform, see <a href="ex1-batch-transform.md">Step 6.2: Deploy the Model with Batch Transform</a>.</p>
<p>If you are using the <a href="https://docs.aws.amazon.com/sagemaker/latest/dg/API_CreateTransformJob.html">CreateTransformJob</a> API, you can reduce the time it takes to complete batch transform jobs by using different parameter values, such as <a href="https://docs.aws.amazon.com/sagemaker/latest/dg/API_CreateTransformJob.html#SageMaker-CreateTransformJob-request-MaxPayloadInMB">MaxPayloadInMB</a>, <a href="https://docs.aws.amazon.com/sagemaker/latest/dg/API_CreateTransformJob.html#SageMaker-CreateTransformJob-request-MaxConcurrentTransforms">MaxConcurrentTransforms</a>, and <a href="https://docs.aws.amazon.com/sagemaker/latest/dg/API_CreateTransformJob.html#SageMaker-CreateTransformJob-request-BatchStrategy">BatchStrategy</a>. Amazon SageMaker automatically finds the optimal parameter settings for built-in algorithms. For custom algorithms, provide these values through an <a href="https://docs.aws.amazon.com/sagemaker/latest/dg/your-algorithms-batch-code.html#your-algorithms-batch-code-how-containe-serves-requests">execution-parameters</a> endpoint.</p>
<p>If you are using the Amazon SageMaker console, you can reduce the time it takes to complete batch transform jobs by using different parameter values, such as <strong>Max payload size (MB)</strong>, <strong>Max concurrent transforms</strong>, and <strong>Batch strategy</strong>, in the <strong>Additional configuration</strong> section of the <strong>Batch transform job configuration</strong> page. Amazon SageMaker automatically finds the optimal parameter settings for built-in algorithms. For custom algorithms, provide these values through an <a href="https://docs.aws.amazon.com/sagemaker/latest/dg/your-algorithms-batch-code.html#your-algorithms-batch-code-how-containe-serves-requests">execution-parameters</a> endpoint.</p>
<p>To test different models or various hyperparameter settings, create a separate transform job for each new model variant and use a validation dataset. For each transform job, specify a unique model name and location in Amazon S3 for the output file. To analyze the results, use <a href="inference-pipeline-logs-metrics.md">Inference Pipeline Logs and Metrics</a>.</p>
<p>Amazon SageMaker uses the Amazon S3 <a href="https://docs.aws.amazon.com/AmazonS3/latest/dev/uploadobjusingmpu.html">Multipart Upload API</a> to upload results from a batch transform job to Amazon S3. If an error occurs, the uploaded results are removed from Amazon S3. In some cases, such as when a network outage occurs, an incomplete multipart upload might remain in Amazon S3. To avoid incurring storage charges, we recommend that you add the <a href="https://docs.aws.amazon.com/AmazonS3/latest/dev/mpuoverview.html#mpu-abort-incomplete-mpu-lifecycle-config">S3 bucket policy</a> to the S3 bucket lifecycle rules. This policy deletes incomplete multipart uploads that might be stored in the S3 bucket. For more information, see <a href="https://docs.aws.amazon.com/AmazonS3/latest/dev/object-lifecycle-mgmt.html">Object Lifecycle Management</a>.</p>
<p>If a batch transform job fails to process an input file because of a problem with the dataset, Amazon SageMaker marks the job as “failed” to alert you. If an input file contains a bad record, the transform job doesn’t create an output file for that input file because it can’t maintain the same order in the transformed data. When your dataset has multiple input files, a transform job continues to process input files even if it fails to process one. The processed files still generate useable results.</p>
<p>Exceeding the <a href="https://docs.aws.amazon.com/sagemaker/latest/dg/API_CreateTransformJob.html#SageMaker-CreateTransformJob-request-MaxPayloadInMB">MaxPayloadInMB</a> limit causes an error. This might happen with a large dataset if it can’t be split, the <a href="https://docs.aws.amazon.com/sagemaker/latest/dg/API_TransformInput.html#SageMaker-Type-TransformInput-SplitType">SplitType</a> parameter is set to <code>none</code>, or individual records within the dataset exceed the limit.</p>
<p>If you are using your own algorithms, you can use placeholder text, such as <code>ERROR</code>, when the algorithm finds a bad record in an input file. For example, if the last record in a dataset is bad, the algorithm should place the error placeholder for that record in the output file.</p>
<p>For a sample notebook that uses batch transform to with a PCA model as a data reduction step on user-item review matrix followed by DBSCAN to cluster movies, see <a href="https://github.com/awslabs/amazon-sagemaker-examples/blob/master/sagemaker_batch_transform/introduction_to_batch_transform/batch_transform_pca_dbscan_movie_clusters.ipynb">https://github.com/awslabs/amazon-sagemaker-examples/blob/master/sagemaker_batch_transform/introduction_to_batch_transform/batch_transform_pca_dbscan_movie_clusters.ipynb</a>. For instructions on creating and accessing Jupyter notebook instances that you can use to run the example in Amazon SageMaker, see <a href="nbi.md">Use Notebook Instances</a>. After creating and opening a notebook instance, choose the <strong>SageMaker Examples</strong> tab to see a list of all the Amazon SageMaker examples. The topic modeling example notebooks that use the NTM algorithms are located in the <strong>Advanced functionality</strong> section. To open a notebook, choose its <strong>Use</strong> tab, then choose <strong>Create copy</strong>.</p>
</body>
</html>
