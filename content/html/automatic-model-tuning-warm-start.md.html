<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>
  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <title>Run a Warm Start Hyperparameter Tuning Job</title>
  <style>
      code{white-space: pre-wrap;}
      span.smallcaps{font-variant: small-caps;}
      span.underline{text-decoration: underline;}
      div.column{display: inline-block; vertical-align: top; width: 50%;}
  </style>
  <!--[if lt IE 9]>
    <script src="//cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7.3/html5shiv-printshiv.min.js"></script>
  <![endif]-->
</head>
<body>
<p>
Copyright Amazon.com, Inc. or its affiliates. All Rights Reserved.<br>SPDX-License-Identifier: CC-BY-SA-4.0
</p>
<header id="title-block-header">
<h1 class="title">Run a Warm Start Hyperparameter Tuning Job<a name="automatic-model-tuning-warm-start"></a></h1>
</header>
<p>Use warm start to start a hyperparameter tuning job using one or more previous tuning jobs as a starting point. The results of previous tuning jobs are used to inform which combinations of hyperparameters to search over in the new tuning job. Hyperparameter tuning uses either Bayesian or random search to choose combinations of hyperparameter values from ranges that you specify. For more information, see <a href="automatic-model-tuning-how-it-works.md">How Hyperparameter Tuning Works</a>. Using information from previous hyperparameter tuning jobs can help increase the performance of the new hyperparameter tuning job by making the search for the best combination of hyperparameters more efficient.</p>
<p><strong>Note</strong><br />
Warm start tuning jobs typically take longer to start than standard hyperparameter tuning jobs, because the results from the parent jobs have to be loaded before the job can start. The increased time depends on the total number of training jobs launched by the parent jobs.</p>
<p>Reasons you might want to consider warm start include: + You want to gradually increase the number of training jobs over several tuning jobs based on the results you see after each iteration. + You get new data, and want to tune a model using the new data. + You want to change the ranges of hyperparameters that you used in a previous tuning job, change static hyperparameters to tunable, or change tunable hyperparameters to static values. + You stopped a previous hyperparameter job early or it stopped unexpectedly.</p>
<p><strong>Topics</strong> + <a href="#tuning-warm-start-types">Types of Warm Start Tuning Jobs</a> + <a href="#warm-start-tuning-restrictions">Warm Start Tuning Restrictions</a> + <a href="#warm-start-tuning-sample-notebooks">Warm Start Tuning Sample Notebook</a> + <a href="#warm-start-tuning-example">Create a Warm Start Tuning Job</a></p>
<p>There are two different types of warm start tuning jobs:</p>
<p><code>IDENTICAL_DATA_AND_ALGORITHM</code><br />
The new hyperparameter tuning job uses the same input data and training image as the parent tuning jobs. You can change the hyperparameter ranges to search and the maximum number of training jobs that the hyperparameter tuning job launches. You can also change hyperparameters from tunable to static, and from static to tunable, but the total number of static plus tunable hyperparameters must remain the same as it is in all parent jobs. You cannot use a new version of the training algorithm, unless the changes in the new version do not affect the algorithm itself. For example, changes that improve logging or adding support for a different data format are allowed.<br />
Use identical data and algorithm when you use the same training data as you used in a previous hyperparameter tuning job, but you want to increase the total number of training jobs or change ranges or values of hyperparameters.<br />
When you run an warm start tuning job of type <code>IDENTICAL_DATA_AND_ALGORITHM</code>, there is an additional field in the response to <a href="API_DescribeHyperParameterTuningJob.md">DescribeHyperParameterTuningJob</a> named <code>OverallBestTrainingJob</code>. The value of this field is the <a href="API_TrainingJobSummary.md">TrainingJobSummary</a> for the training job with the best objective metric value of all training jobs launched by this tuning job and all parent jobs specified for the warm start tuning job.</p>
<p><code>TRANSFER_LEARNING</code><br />
The new hyperparameter tuning job can include input data, hyperparameter ranges, maximum number of concurrent training jobs, and maximum number of training jobs that are different than those of its parent hyperparameter tuning jobs. You can also change hyperparameters from tunable to static, and from static to tunable, but the total number of static plus tunable hyperparameters must remain the same as it is in all parent jobs. The training algorithm image can also be a different version from the version used in the parent hyperparameter tuning job. When you use transfer learning, changes in the dataset or the algorithm that significantly affect the value of the objective metric might reduce the usefulness of using warm start tuning.</p>
<p>The following restrictions apply to all warm start tuning jobs: + A tuning job can have a maximum of 5 parent jobs, and all parent jobs must be in a terminal state (<code>Completed</code>, <code>Stopped</code>, or <code>Failed</code>) before you start the new tuning job. + The objective metric used in the new tuning job must be the same as the objective metric used in the parent jobs. + The total number of static plus tunable hyperparameters must remain the same between parent jobs and the new tuning job. Because of this, if you think you might want to use a hyperparameter as tunable in a future warm start tuning job, you should add it as a static hyperparameter when you create a tuning job. + The type of each hyperparameter (continuous, integer, categorical) must not change between parent jobs and the new tuning job. + The number of total changes from tunable hyperparameters in the parent jobs to static hyperparameters in the new tuning job, plus the number of changes in the values of static hyperparameters cannot be more than 10. Each value in a categorical hyperparameter counts against this limit. For example, if the parent job has a tunable categorical hyperparameter with the possible values <code>red</code> and <code>blue</code>, you change that hyperparameter to static in the new tuning job, that counts as 2 changes against the allowed total of 10. If the same hyperparameter had a static value of <code>red</code> in the parent job, and you change the static value to <code>blue</code> in the new tuning job, it also counts as 2 changes. + Warm start tuning is not recursive. For example, if you create <code>MyTuningJob3</code> as a warm start tuning job with <code>MyTuningJob2</code> as a parent job, and <code>MyTuningJob2</code> is itself an warm start tuning job with a parent job <code>MyTuningJob1</code>, the information that was learned when running <code>MyTuningJob1</code> is not used for <code>MyTuningJob3</code>. If you want to use the information from <code>MyTuningJob1</code>, you must explicitly add it as a parent for <code>MyTuningJob3</code>. + The training jobs launched by every parent job in a warm start tuning job count against the 500 maximum training jobs for a tuning job. + Hyperparameter tuning jobs created before October 1, 2018 cannot be used as parent jobs for warm start tuning jobs.</p>
<p>For a sample notebook that shows how to use warm start tuning, see <a href="https://github.com/awslabs/amazon-sagemaker-examples/blob/master/hyperparameter_tuning/image_classification_warmstart/hpo_image_classification_warmstart.ipynb">https://github.com/awslabs/amazon-sagemaker-examples/blob/master/hyperparameter_tuning/image_classification_warmstart/hpo_image_classification_warmstart.ipynb</a>. For instructions how to create and access Jupyter notebook instances that you can use to run the example in Amazon SageMaker, see <a href="howitworks-nbexamples.md">Use Example Notebooks</a>. Once you have created a notebook instance and opened it, select the <strong>SageMaker Examples</strong> tab to see a list of all the Amazon SageMaker samples. The warm start tuning example notebook is located in the <strong>Hyperparameter tuning</strong> section, and is named <code>hpo_image_classification_warmstart.ipynb</code>. To open a notebook, click on its <strong>Use</strong> tab and select <strong>Create copy</strong>.</p>
<p>You can use either the low-level AWS SDK for Python (Boto 3) or the high-level Amazon SageMaker Python SDK to create a warm start tuning job.</p>
<p><strong>Topics</strong> + <a href="#warm-start-tuning-example-boto">Create a Warm Start Tuning Job ( Low-level Amazon SageMaker API for Python (Boto 3))</a> + <a href="#warm-start-tuning-example-sdk">Create a Warm Start Tuning Job (Amazon SageMaker Python SDK)</a></p>
<p>To use warm start tuning, you specify the values of a <a href="API_HyperParameterTuningJobWarmStartConfig.md">HyperParameterTuningJobWarmStartConfig</a> object, and pass that as the <code>WarmStartConfig</code> field in a call to <a href="API_CreateHyperParameterTuningJob.md">CreateHyperParameterTuningJob</a>.</p>
<p>The following code shows how to create a <a href="API_HyperParameterTuningJobWarmStartConfig.md">HyperParameterTuningJobWarmStartConfig</a> object and pass it to <a href="API_CreateHyperParameterTuningJob.md">CreateHyperParameterTuningJob</a> job by using the low-level Amazon SageMaker API for Python (Boto 3).</p>
<p>Create the <code>HyperParameterTuningJobWarmStartConfig</code> object:</p>
<pre><code>warm_start_config = {
          &quot;ParentHyperParameterTuningJobs&quot; : [ 
          {&quot;HyperParameterTuningJobName&quot; : &#39;MyParentTuningJob&#39;}
          ],
          &quot;WarmStartType&quot; : &quot;IdenticalDataAndAlgorithm&quot;    
}</code></pre>
<p>Create the warm start tuning job:</p>
<pre><code>smclient = boto3.Session().client(&#39;sagemaker&#39;)
smclient.create_hyper_parameter_tuning_job(HyperParameterTuningJobName = &#39;MyWarmStartTuningJob&#39;,
   HyperParameterTuningJobConfig = tuning_job_config, # See notebook for tuning configuration
   TrainingJobDefinition = training_job_definition, # See notebook for job definition
   WarmStartConfig = warm_start_config)</code></pre>
<p>To use the Amazon SageMaker Python SDK to run a warm start tuning job, you: + Specify the parent jobs and the warm start type by using a <code>WarmStartConfig</code> object. + Pass the <code>WarmStartConfig</code> object as the value of the <code>warm_start_config</code> argument of a <a href="https://sagemaker.readthedocs.io/en/latest/tuner.html">HyperparameterTuner</a> object. + Call the <code>fit</code> method of the <code>HyperparameterTuner</code> object.</p>
<p>For more information about using the Amazon SageMaker Python SDK for hyperparameter tuning, see <a href="https://github.com/aws/sagemaker-python-sdk#sagemaker-automatic-model-tuning">https://github.com/aws/sagemaker-python-sdk#sagemaker-automatic-model-tuning</a>.</p>
<p>This example uses an estimator that uses the <a href="image-classification.md">Image Classification Algorithm</a> algorithm for training. The following code sets the hyperparameter ranges that the warm start tuning job searches within to find the best combination of values. For information about setting hyperparameter ranges, see <a href="automatic-model-tuning-define-ranges.md">Define Hyperparameter Ranges</a>.</p>
<pre><code>hyperparameter_ranges = {&#39;learning_rate&#39;: ContinuousParameter(0.0, 0.1),
                         &#39;momentum&#39;: ContinuousParameter(0.0, 0.99)}</code></pre>
<p>The following code configures the warm start tuning job by creating a <code>WarmStartConfig</code> object.</p>
<pre><code>from sagemaker.tuner import WarmStartConfig,
          WarmStartTypes

parent_tuning_job_name = &quot;MyParentTuningJob&quot;
warm_start_config = WarmStartConfig(type=WarmStartTypes.IDENTICAL_DATA_AND_ALGORITHM, parents={parent_tuning_job_name})</code></pre>
<p>Now set the values for static hyperparameters, which are hyperparameters that keep the same value for every training job that the warm start tuning job launches. In the following code, <code>imageclassification</code> is an estimator that was created previously.</p>
<pre><code>imageclassification.set_hyperparameters(num_layers=18,
                                        image_shape=&#39;3,224,224&#39;,
                                        num_classes=257,
                                        num_training_samples=15420,
                                        mini_batch_size=128,
                                        epochs=30,
                                        optimizer=&#39;sgd&#39;,
                                        top_k=&#39;2&#39;,
                                        precision_dtype=&#39;float32&#39;,
                                        augmentation_type=&#39;crop&#39;)</code></pre>
<p>Now create the <code>HyperparameterTuner</code> object and pass the <code>WarmStartConfig</code> object that you previously created as the <code>warm_start_config</code> argument.</p>
<pre><code>tuner_warm_start = HyperparameterTuner(imageclassification,
                            &#39;validation:accuracy&#39;,
                            hyperparameter_ranges,
                            objective_type=&#39;Maximize&#39;,
                            max_jobs=10,
                            max_parallel_jobs=2,
                            base_tuning_job_name=&#39;warmstart&#39;,
                            warm_start_config=warm_start_config)</code></pre>
<p>Finally, call the <code>fit</code> method of the <code>HyperparameterTuner</code> object to launch the warm start tuning job.</p>
<pre><code>tuner_warm_start.fit(
        {&#39;train&#39;: s3_input_train, &#39;validation&#39;: s3_input_validation},
        include_cls_metadata=False)</code></pre>
</body>
</html>
