<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>
  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <title>How Hyperparameter Tuning Works</title>
  <style>
      code{white-space: pre-wrap;}
      span.smallcaps{font-variant: small-caps;}
      span.underline{text-decoration: underline;}
      div.column{display: inline-block; vertical-align: top; width: 50%;}
  </style>
  <!--[if lt IE 9]>
    <script src="//cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7.3/html5shiv-printshiv.min.js"></script>
  <![endif]-->
</head>
<body>
<p>
Copyright Amazon.com, Inc. or its affiliates. All Rights Reserved.<br>SPDX-License-Identifier: CC-BY-SA-4.0
</p>
<header id="title-block-header">
<h1 class="title">How Hyperparameter Tuning Works<a name="automatic-model-tuning-how-it-works"></a></h1>
</header>
<p>In a random search, hyperparameter tuning chooses a random combination of values from within the ranges that you specify for hyperparameters for each training job it launches. Because the choice of hyperparameter values doesn’t depend on the results of previous training jobs, you can run the maximum number of concurrent training jobs without affecting the performance of the search.</p>
<p>For an example notebook that uses random search, see <a href="https://github.com/awslabs/amazon-sagemaker-examples/blob/master/hyperparameter_tuning/xgboost_random_log/hpo_xgboost_random_log.ipynb">https://github.com/awslabs/amazon-sagemaker-examples/blob/master/hyperparameter_tuning/xgboost_random_log/hpo_xgboost_random_log.ipynb</a>.</p>
<p>Bayesian search treats hyperparameter tuning like a <em><a href="https://docs.aws.amazon.com/general/latest/gr/glos-chap.html#%5Bregression%5D">[regression]</a></em> problem. Given a set of input features (the hyperparameters), hyperparameter tuning optimizes a model for the metric that you choose. To solve a regression problem, hyperparameter tuning makes guesses about which hyperparameter combinations are likely to get the best results, and runs training jobs to test these values. After testing the first set of hyperparameter values, hyperparameter tuning uses regression to choose the next set of hyperparameter values to test.</p>
<p>Hyperparameter tuning uses an Amazon SageMaker implementation of Bayesian optimization.</p>
<p>When choosing the best hyperparameters for the next training job, hyperparameter tuning considers everything that it knows about this problem so far. Sometimes it chooses a combination of hyperparameter values close to the combination that resulted in the best previous training job to incrementally improve performance. This allows hyperparameter tuning to exploit the best known results. Other times, it chooses a set of hyperparameter values far removed from those it has tried. This allows it to explore the range of hyperparameter values to try to find new areas that are not well understood. The explore/exploit trade-off is common in many machine learning problems.</p>
<p>For more information about Bayesian optimization, see the following:</p>
<p><strong>Basic Topics on Bayesian Optimization</strong> + <a href="https://arxiv.org/abs/1012.2599">A Tutorial on Bayesian Optimization of Expensive Cost Functions, with Application to Active User Modeling and Hierarchical Reinforcement Learning</a> + <a href="https://arxiv.org/abs/1206.2944">Practical Bayesian Optimization of Machine Learning Algorithms</a> + <a href="http://ieeexplore.ieee.org/document/7352306/?reload=true">Taking the Human Out of the Loop: A Review of Bayesian Optimization</a></p>
<p><strong>Speeding up Bayesian Optimization</strong> + <a href="https://liamcli.com/assets/pdf/hyperband_jmlr.pdf">Hyperband: A Novel Bandit-Based Approach to Hyperparameter Optimization</a> + <a href="https://dl.acm.org/citation.cfm?id=3098043">Google Vizier: A Service for Black-Box Optimization</a> + <a href="https://openreview.net/forum?id=S11KBYclx">Learning Curve Prediction with Bayesian Neural Networks</a> + <a href="https://dl.acm.org/citation.cfm?id=2832731">Speeding up automatic hyperparameter optimization of deep neural networks by extrapolation of learning curves</a></p>
<p><strong>Advanced Modeling and Transfer Learning</strong> + <a href="https://papers.nips.cc/paper/7917-scalable-hyperparameter-transfer-learning">Scalable Hyperparameter Transfer Learning</a> + <a href="http://proceedings.mlr.press/v70/jenatton17a.html">Bayesian Optimization with Tree-structured Dependencies</a> + <a href="https://papers.nips.cc/paper/6116-bayesian-optimization-with-robust-bayesian-neural-networks">Bayesian Optimization with Robust Bayesian Neural Networks</a> + <a href="http://proceedings.mlr.press/v37/snoek15.pdf">Scalable Bayesian Optimization Using Deep Neural Networks</a> + <a href="https://arxiv.org/abs/1402.0929">Input Warping for Bayesian Optimization of Non-stationary Functions</a></p>
<p><strong>Note</strong><br />
Hyperparameter tuning might not improve your model. It is an advanced tool for building machine solutions, and, as such, should be considered part of the scientific development process.<br />
When you build complex machine learning systems like deep learning neural networks, exploring all of the possible combinations is impractical. Hyperparameter tuning can accelerate your productivity by trying many variations of a model, focusing on the most promising combinations of hyperparameter values within the ranges that you specify. To get good results, you need to choose the right ranges to explore. Because the algorithm itself is stochastic, it’s possible that the hyperparameter tuning model will fail to converge on the best answer, even if the best possible combination of values is within the ranges that you choose.</p>
</body>
</html>
